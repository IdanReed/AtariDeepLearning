{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0221f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91df5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import seed_random_generators\n",
    "seed_random_generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e551eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1880 episodes\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from npz_loader import load_episodes\n",
    "\n",
    "dataset_root = Path(\"dataset\")\n",
    "holdout_game_dirs = [\n",
    "    dataset_root.joinpath(\"BeamRiderNoFrameskip-v4\", \"BeamRiderNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"BreakoutNoFrameskip-v4\", \"BreakoutNoFrameskip-v4\")\n",
    "]\n",
    "\n",
    "main_game_dirs = [\n",
    "    dataset_root.joinpath(\"EnduroNoFrameskip-v4\", \"EnduroNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"MsPacmanNoFrameskip-v4\", \"MsPacmanNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"PongNoFrameskip-v4\", \"PongNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"QbertNoFrameskip-v4\", \"QbertNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"SeaquestNoFrameskip-v4\", \"SeaquestNoFrameskip-v4\"),\n",
    "    dataset_root.joinpath(\"SpaceInvadersNoFrameskip-v4\", \"SpaceInvadersNoFrameskip-v4\")\n",
    "]\n",
    "episodes = load_episodes(main_game_dirs, holdout_game_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523cdbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1880 items (100.0% of 1880 total)\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_list\n",
    "sampled_episodes = sample_list(episodes, fraction=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae8e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epsiode_dataloader import make_train_val_dataloaders\n",
    "\n",
    "main_bundle, holdout_bundle, bins = make_train_val_dataloaders(\n",
    "    episodes=sampled_episodes,\n",
    "    holdout_game_dirs=holdout_game_dirs,\n",
    "    train_frac=0.8,\n",
    "\n",
    "    # We should experiment with this, but it throws off steps being equal in terms of tokens/timesteps seen\n",
    "    # So I think we keep it as some fixed number for all experiments except for an experiment specifically looking at it\n",
    "    timestep_window_size=4, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc9d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfedb9",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d499c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kenny started this, might just steal those\n",
    "# Some experiments like freeze below should just use the best params from baseline since there's no changes to the original model,\n",
    "# but other experiments should find their own best params if there are changes to the model (like patch vs CNN)\n",
    "best_params = {\n",
    "    'lr': 0.002226768831180977,\n",
    "    'emb_size': 128,\n",
    "    'n_layers': 2,\n",
    "    'n_heads': 2,\n",
    "    'num_epochs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c123937",
   "metadata": {},
   "source": [
    "# Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc3f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 23:17:32,436] A new study created in memory with name: no-name-40fa0255-b8b1-4f58-aba7-545f3274c1ef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared 0 files from output\\freeze_transformer\n",
      "Trial params: {'lr': 4.105152517595741e-05, 'emb_size': 128, 'n_layers': 5, 'n_heads': 2, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 41734/41734 [31:41<00:00, 21.94it/s]   \n",
      "Epoch 2/4: 100%|██████████| 41734/41734 [23:45<00:00, 29.28it/s]   \n",
      "Epoch 3/4: 100%|██████████| 41734/41734 [28:38<00:00, 24.28it/s]   \n",
      "Epoch 4/4: 100%|██████████| 41734/41734 [24:05<00:00, 28.88it/s]   \n",
      "[I 2025-12-09 01:25:58,686] Trial 0 finished with value: 1.585356593132019 and parameters: {'lr': 4.105152517595741e-05, 'emb_size': 128, 'n_layers': 5, 'n_heads': 2, 'num_epochs': 4}. Best is trial 0 with value: 1.585356593132019.\n",
      "[I 2025-12-09 01:25:58,688] Trial 1 pruned. \n",
      "[I 2025-12-09 01:25:58,689] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 0.0046438104160003, 'emb_size': 128, 'n_layers': 6, 'n_heads': 3, 'num_epochs': 4}\n",
      "Trial params: {'lr': 3.863842464855461e-05, 'emb_size': 512, 'n_layers': 3, 'n_heads': 3, 'num_epochs': 2}\n",
      "Trial params: {'lr': 0.0017241299059688473, 'emb_size': 128, 'n_layers': 2, 'n_heads': 1, 'num_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 41734/41734 [20:44<00:00, 33.53it/s]   \n",
      "Epoch 2/2: 100%|██████████| 41734/41734 [21:02<00:00, 33.05it/s]   \n",
      "[I 2025-12-09 02:16:45,274] Trial 3 finished with value: 1.6078293323516846 and parameters: {'lr': 0.0017241299059688473, 'emb_size': 128, 'n_layers': 2, 'n_heads': 1, 'num_epochs': 2}. Best is trial 0 with value: 1.585356593132019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 0.000253510289222719, 'emb_size': 512, 'n_layers': 6, 'n_heads': 2, 'num_epochs': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 41734/41734 [33:03<00:00, 21.04it/s]   \n",
      "Epoch 2/2: 100%|██████████| 41734/41734 [33:05<00:00, 21.01it/s]   \n",
      "[I 2025-12-09 03:31:42,816] Trial 4 finished with value: 1.713098168373108 and parameters: {'lr': 0.000253510289222719, 'emb_size': 512, 'n_layers': 6, 'n_heads': 2, 'num_epochs': 2}. Best is trial 0 with value: 1.585356593132019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 0.0004912304743169713, 'emb_size': 512, 'n_layers': 4, 'n_heads': 2, 'num_epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 41734/41734 [26:11<00:00, 26.57it/s]   \n",
      "[I 2025-12-09 04:02:20,166] Trial 5 finished with value: 1.7166712284088135 and parameters: {'lr': 0.0004912304743169713, 'emb_size': 512, 'n_layers': 4, 'n_heads': 2, 'num_epochs': 1}. Best is trial 0 with value: 1.585356593132019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial params: {'lr': 5.940765476348109e-05, 'emb_size': 1024, 'n_layers': 5, 'n_heads': 4, 'num_epochs': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|██████████| 41734/41734 [47:42<00:00, 14.58it/s]   \n",
      "Epoch 2/9: 100%|██████████| 41734/41734 [47:42<00:00, 14.58it/s]   \n",
      "Epoch 3/9: 100%|██████████| 41734/41734 [47:44<00:00, 14.57it/s]   \n",
      "Epoch 4/9: 100%|██████████| 41734/41734 [47:40<00:00, 14.59it/s]   \n",
      "Epoch 5/9:  83%|████████▎ | 34515/41734 [39:17<08:13, 14.64it/s]   \n",
      "[W 2025-12-09 08:10:48,214] Trial 6 failed with parameters: {'lr': 5.940765476348109e-05, 'emb_size': 1024, 'n_layers': 5, 'n_heads': 4, 'num_epochs': 9} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\optuna_tuning.py\", line 33, in objective\n",
      "    model, main_train_stats, main_val_stats = train_mgdt(\n",
      "                                              ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model_trainer.py\", line 147, in train_mgdt\n",
      "    out, loss, stats = model.forward_and_compute_loss(frames, rtg_bins, actions, reward_bins, game_ids)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model.py\", line 280, in forward_and_compute_loss\n",
      "    out = self.forward(frames, rtg_bins, actions, reward_bins, game_ids)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model.py\", line 261, in forward\n",
      "    return_logits = self.return_head(return_h)   # (B, T, n_return_bins)\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1951, in __getattr__\n",
      "    def __getattr__(self, name: str) -> Union[Tensor, \"Module\"]:\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2025-12-09 08:10:48,255] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexperiment_freeze\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_experiment_freeze\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmgdt_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Freezeable\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m freeze_params = \u001b[43mrun_experiment_freeze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFreeze Transformer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_bundle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mholdout_bundle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mholdout_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFreezeable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTransformer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfreeze_transformer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# switch to take best params from baseline later\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\experiment_freeze.py:24\u001b[39m, in \u001b[36mrun_experiment_freeze\u001b[39m\u001b[34m(title_prefix, main_bundle, holdout_bundle, bins, freeze_components, experiment_dir, best_params)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna_tuning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_optuna\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     study = \u001b[43mrun_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_bundle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_bundle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     best_params = study.best_params\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Train with best params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\optuna_tuning.py:47\u001b[39m, in \u001b[36mrun_optuna\u001b[39m\u001b[34m(train_loader, val_loader, bins, n_trials, lr_range, emb_size_choices, n_layers_range, n_heads_range, num_epochs_range, encoder_type)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m main_val_stats[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     46\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\optuna_tuning.py:33\u001b[39m, in \u001b[36mrun_optuna.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m emb_size % n_heads != \u001b[32m0\u001b[39m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m optuna.exceptions.TrialPruned()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model, main_train_stats, main_val_stats = \u001b[43mtrain_mgdt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43memb_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_type\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m main_val_stats[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model_trainer.py:147\u001b[39m, in \u001b[36mtrain_mgdt\u001b[39m\u001b[34m(bins, dataloader_train, dataloader_val, model, num_epochs, val_every_pct, mid_epoch_val_fraction, val_seed, encoder_type, image_size, emb_size, n_layers, n_heads, max_timestep_window_size, lr, finetune_lr_factor, weight_decay, device)\u001b[39m\n\u001b[32m    144\u001b[39m optimizer.zero_grad()\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m out, loss, stats = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_and_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtg_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m    150\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model.py:280\u001b[39m, in \u001b[36mMGDTModel.forward_and_compute_loss\u001b[39m\u001b[34m(self, frames, rtg_bins, actions, reward_bins, game_ids)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_and_compute_loss\u001b[39m(\n\u001b[32m    272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    273\u001b[39m     frames: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m     game_ids: torch.Tensor\n\u001b[32m    278\u001b[39m ) -> Tuple[torch.Tensor, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtg_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     B, T = actions.shape\n\u001b[32m    284\u001b[39m     loss_R = F.cross_entropy(\n\u001b[32m    285\u001b[39m         out[\u001b[33m\"\u001b[39m\u001b[33mreturn_logits\u001b[39m\u001b[33m\"\u001b[39m].view(B * T, -\u001b[32m1\u001b[39m),\n\u001b[32m    286\u001b[39m         rtg_bins.view(-\u001b[32m1\u001b[39m),\n\u001b[32m    287\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\mgdt_model.py:261\u001b[39m, in \u001b[36mMGDTModel.forward\u001b[39m\u001b[34m(self, frames, rtg_bins, actions, reward_bins, game_ids)\u001b[39m\n\u001b[32m    258\u001b[39m action_h = hidden_states[:, src_act, :]\n\u001b[32m    259\u001b[39m reward_h = hidden_states[:, src_rew, :]\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m return_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_head\u001b[49m(return_h)   \u001b[38;5;66;03m# (B, T, n_return_bins)\u001b[39;00m\n\u001b[32m    262\u001b[39m action_logits = \u001b[38;5;28mself\u001b[39m.action_head(action_h)   \u001b[38;5;66;03m# (B, T, n_actions)\u001b[39;00m\n\u001b[32m    263\u001b[39m reward_logits = \u001b[38;5;28mself\u001b[39m.reward_head(reward_h)   \u001b[38;5;66;03m# (B, T, n_reward_bins)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\idanc\\local\\projects\\AtariDeepLearning\\.venv_atari\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1951\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1946\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1953\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from experiment_freeze import run_experiment_freeze\n",
    "from mgdt_model import Freezeable\n",
    "\n",
    "freeze_params = run_experiment_freeze(\n",
    "    title_prefix=\"Freeze Transformer\",\n",
    "    main_bundle=main_bundle,\n",
    "    holdout_bundle=holdout_bundle,\n",
    "    bins=bins,\n",
    "    freeze_components=[Freezeable.Transformer],\n",
    "    experiment_dir=base_dir.joinpath(\"freeze_transformer\"),\n",
    ")  # switch to take best params from baseline later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79033b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_experiment_freeze(\n",
    "    title_prefix=\"Freeze Obs Encoder\",\n",
    "    main_bundle=main_bundle,\n",
    "    holdout_bundle=holdout_bundle,\n",
    "    bins=bins,\n",
    "    freeze_components=[Freezeable.ObsEncoder],\n",
    "    experiment_dir=base_dir.joinpath(\"freeze_obs_encoder\"),\n",
    "    best_params=freeze_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21c672",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_basic import run_experiment_basic\n",
    "from mgdt_model_trainer import Encoder\n",
    "\n",
    "_ = run_experiment_basic(\n",
    "    \"CNN\",\n",
    "    main_bundle,\n",
    "    holdout_bundle,\n",
    "    bins,\n",
    "    base_dir.joinpath(\"cnn\"),\n",
    "    encoder_type=Encoder.CNN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd4b21",
   "metadata": {},
   "source": [
    "# Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac020836",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bundle_window_8, holdout_bundle_window_8, bins_window_8 = make_train_val_dataloaders(\n",
    "    episodes=sampled_episodes,\n",
    "    holdout_game_dirs=holdout_game_dirs,\n",
    "    train_frac=0.8,\n",
    "    timestep_window_size=8, \n",
    ")\n",
    "_ = run_experiment_basic(\n",
    "    \"Window Size 8\",\n",
    "    main_bundle_window_8,\n",
    "    holdout_bundle_window_8,\n",
    "    bins_window_8,\n",
    "    base_dir.joinpath(\"window_size_8\"),\n",
    "    encoder_type=Encoder.Patch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e68ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bundle_window_16, holdout_bundle_window_16, bins_window_16 = make_train_val_dataloaders(\n",
    "    episodes=sampled_episodes,\n",
    "    holdout_game_dirs=holdout_game_dirs,\n",
    "    train_frac=0.8,\n",
    "    timestep_window_size=16, \n",
    ")\n",
    "_ = run_experiment_basic(\n",
    "    \"Window Size 16\",\n",
    "    main_bundle_window_16,\n",
    "    holdout_bundle_window_16,\n",
    "    bins_window_16,\n",
    "    base_dir.joinpath(\"window_size_16\"),\n",
    "    encoder_type=Encoder.Patch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bundle_window_32, holdout_bundle_window_32, bins_window_32 = make_train_val_dataloaders(\n",
    "    episodes=sampled_episodes,\n",
    "    holdout_game_dirs=holdout_game_dirs,\n",
    "    train_frac=0.8,\n",
    "    timestep_window_size=32, \n",
    ")\n",
    "_ = run_experiment_basic(\n",
    "    \"Window Size 32\",\n",
    "    main_bundle_window_32,\n",
    "    holdout_bundle_window_32,\n",
    "    bins_window_32,\n",
    "    base_dir.joinpath(\"window_size_32\"),\n",
    "    encoder_type=Encoder.Patch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64871c9d",
   "metadata": {},
   "source": [
    "# Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as baseline, but just in case something goes wrong with kenny's run\n",
    "# Set to run last in case this takes >1 day\n",
    "from experiment_basic import run_experiment_basic\n",
    "from mgdt_model_trainer import Encoder\n",
    "\n",
    "# _ = run_experiment_basic(\n",
    "#     \"Patch\",\n",
    "#     main_bundle,\n",
    "#     holdout_bundle,\n",
    "#     bins,\n",
    "#     base_dir.joinpath(\"patch\"),\n",
    "#     encoder_type=Encoder.Patch,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a88be7",
   "metadata": {},
   "source": [
    "# Comparison \n",
    "## *Keep this at bottom of notebook and add new experiments to it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_checkpoint\n",
    "from mgdt_model_stats import ExperimentData\n",
    "\n",
    "def load_experiment_data(name: str, output_dir: Path) -> ExperimentData:\n",
    "    checkpoint = load_checkpoint(output_dir)\n",
    "    return ExperimentData(\n",
    "        name=name,\n",
    "        main_train_stats=checkpoint.main_train_stats,\n",
    "        main_val_stats=checkpoint.main_val_stats,\n",
    "        holdout_train_stats=checkpoint.holdout_train_stats,\n",
    "        holdout_val_stats=checkpoint.holdout_val_stats,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mgdt_model_stats import experiment_comparison\n",
    "\n",
    "experiments = [\n",
    "    load_experiment_data(\"Freeze Transformer\", base_dir.joinpath(\"freeze_transformer\")),\n",
    "    load_experiment_data(\"Freeze Obs Encoder\", base_dir.joinpath(\"freeze_obs_encoder\")),\n",
    "    load_experiment_data(\"CNN\", base_dir.joinpath(\"cnn\")),\n",
    "    load_experiment_data(\"Window Size 8\", base_dir.joinpath(\"window_size_8\")),\n",
    "    load_experiment_data(\"Window Size 16\", base_dir.joinpath(\"window_size_16\")),\n",
    "    load_experiment_data(\"Window Size 32\", base_dir.joinpath(\"window_size_32\")),\n",
    "    load_experiment_data(\"Patch\", base_dir.joinpath(\"patch\")),\n",
    "]\n",
    "\n",
    "experiment_comparison(experiments, output_dir=base_dir.joinpath(\"experiment_comparison\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f2917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
